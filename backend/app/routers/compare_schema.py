from __future__ import annotations

from datetime import datetime, timezone

from fastapi import APIRouter, Depends, HTTPException, status

from ..deps import get_current_user
from ..models import CompareModelsRequest
from ..services.compare_engine import (
    compare_schemas as compare_schema_results,
    compare_schemas_detailed,
    generate_schema_for_comparison,
    validate_model_name,
)


router = APIRouter(prefix="/schemas", tags=["schemas"])


@router.post("/compare-models")
async def compare_models(payload: CompareModelsRequest, current_user=Depends(get_current_user)):
    """Generate schemas using different AI models and compare them - uses compare_engine."""
    try:
        model1_name = payload.model_1.lower()
        model2_name = payload.model_2.lower()

        # Validate model names
        if not validate_model_name(model1_name):
            raise ValueError(f"Invalid model: {model1_name}")
        if not validate_model_name(model2_name):
            raise ValueError(f"Invalid model: {model2_name}")

        # Generate schemas with specified models using compare_engine
        result1 = generate_schema_for_comparison(payload.requirement, payload.workload_type, model1_name)
        result2 = generate_schema_for_comparison(payload.requirement, payload.workload_type, model2_name)

        # Use compare_engine comparison logic (basic and detailed)
        comparison = compare_schema_results(result1, result2)
        detailed_comparison = compare_schemas_detailed(result1, result2)

        # Extract collections for analysis
        s1_collections = set(result1.get("schema", {}).keys())
        s2_collections = set(result2.get("schema", {}).keys())
        only_in_1 = comparison["onlyIn1"]
        only_in_2 = comparison["onlyIn2"]
        common = comparison["common"]
        similarity_score = comparison["similarityScore"]

        # Format model names for display
        model1_display = model1_name.upper()
        model2_display = model2_name.upper()

        # Build analysis
        analysis = f"""
AI Model Comparison Analysis
============================

Requirement: {payload.requirement[:200]}...
Workload Type: {payload.workload_type}

Collections Generated by Each Model:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
{model1_display} (Model 1) collections: {len(s1_collections)}
{model2_display} (Model 2) collections: {len(s2_collections)}
Common collections: {len(common)}

Collections only in {model1_display}:
{chr(10).join(f"  • {col}" for col in only_in_1) if only_in_1 else "  (none)"}

Collections only in {model2_display}:
{chr(10).join(f"  • {col}" for col in only_in_2) if only_in_2 else "  (none)"}

Common collections (both models agree):
{chr(10).join(f"  • {col}" for col in common) if common else "  (none)"}

Design Patterns:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Similarity Score: {similarity_score:.1f}%

{f"Analysis Focus:{chr(10)}{payload.analysis_text}" if payload.analysis_text else ""}
"""

        return {
            "model1": {
                "_id": f"generated-{model1_name}",
                "model": model1_name,
                "inputText": payload.requirement,
                "workloadType": payload.workload_type,
                "result": result1,
                "createdAt": datetime.now(timezone.utc).isoformat(),
            },
            "model2": {
                "_id": f"generated-{model2_name}",
                "model": model2_name,
                "inputText": payload.requirement,
                "workloadType": payload.workload_type,
                "result": result2,
                "createdAt": datetime.now(timezone.utc).isoformat(),
            },
            "analysis": analysis.strip(),
            "comparison": {
                "onlyIn1": only_in_1,
                "onlyIn2": only_in_2,
                "common": common,
                "similarityScore": similarity_score,
            },
            "detailedComparison": detailed_comparison,
        }
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Model comparison failed: {str(e)}",
        )
